{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "6aypZKArEVQn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vridhi-Wadhawan/rag-politics-qa-llamaindex/blob/main/rag_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Question Answering (RAG) Pipeline\n",
        "\n",
        "This notebook implements an end-to-end Retrieval-Augmented Question Answering (RAG) system to answer fact-based political questions using Wikipedia data on Indian Prime Ministers.\n",
        "\n",
        "The focus is on retrieval quality, system-level enhancements, and evaluation\n",
        "rather than model fine-tuning.\n",
        "\n",
        "> This project focuses on retrieval and inference-time system design rather than model fine-tuning.\n"
      ],
      "metadata": {
        "id": "6aypZKArEVQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Overview\n",
        "\n",
        "1. Wikipedia pages are scraped and stored as Prime Minister–level documents  \n",
        "2. Documents are chunked into overlapping segments and embedded into a dense vector index  \n",
        "3. Queries retrieve relevant chunks using similarity search  \n",
        "4. Retrieved context is enhanced via reranking and query expansion  \n",
        "5. Answers are generated using prompt ensembles  \n",
        "6. Outputs are evaluated using lexical and semantic metrics\n"
      ],
      "metadata": {
        "id": "u-R_pQYJkJGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Ingestion\n",
        "\n",
        "Wikipedia pages of all Indian Prime Ministers are collected and stored as documents. Each document is tagged with metadata identifying the Prime Minister, which is later used for retrieval analysis.\n",
        "\n",
        "**Corpus coverage:**\n",
        "Jawaharlal Nehru, Lal Bahadur Shastri, Indira Gandhi, Morarji Desai, Charan Singh,\n",
        "Rajiv Gandhi, V. P. Singh, Chandra Shekhar, P. V. Narasimha Rao, H. D. Deve Gowda,\n",
        "I. K. Gujral, Atal Bihari Vajpayee, Manmohan Singh, and Narendra Modi.\n"
      ],
      "metadata": {
        "id": "6yWahcaEe-g1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhixZn6V_d-N"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------\n",
        "# Importing Libraries\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# Core\n",
        "import os, re, random, torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# NLP & Data\n",
        "import wikipediaapi\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# LLMs & Embeddings\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# LlamaIndex\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    Document,\n",
        "    Settings,\n",
        "    StorageContext,\n",
        "    load_index_from_storage)\n",
        "from llama_index.core.text_splitter import TokenTextSplitter\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install and import all required dependencies such as llama-index, transformers, sentence-transformers, spacy, and nltk which are then used for LLM inference, semantic similarity, text preprocessing, and keyword extraction."
      ],
      "metadata": {
        "id": "Xo-odJmyBrps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Downloading nltk resources (one-time)\n",
        "# ------------------------------------------------------\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mp4rB4Jsekah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Loading spaCy small English model for quick NER / POS\n",
        "# ------------------------------------------------------\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "i8aoT4lqem2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Scraping Wikipedia Pages\n",
        "# ------------------------------------------------------\n",
        "wiki = wikipediaapi.Wikipedia(user_agent='MyWikipediaApp/1.0 (example@domain.com)', language='en')\n",
        "\n",
        "pm_names = [\"Jawaharlal Nehru\", \"Lal Bahadur Shastri\", \"Indira Gandhi\",\"Morarji Desai\", \"Charan Singh\", \"Rajiv Gandhi\", \"V. P. Singh\",\n",
        "            \"Chandra Shekhar\", \"P. V. Narasimha Rao\", \"H. D. Deve Gowda\", \"I. K. Gujral\", \"Atal Bihari Vajpayee\", \"Manmohan Singh\", \"Narendra Modi\"]\n",
        "\n",
        "documents = []\n",
        "for name in pm_names:\n",
        "    page = wiki.page(name)\n",
        "    if page.exists():\n",
        "        documents.append(page.text)\n",
        "    else:\n",
        "        print(f\"Page not found: {name}\")\n",
        "\n",
        "print(f\"\\n Successfully downloaded Wikipedia pages for {len(documents)} Prime Ministers.\")"
      ],
      "metadata": {
        "id": "TiMe_bD_fH5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing Pipeline\n",
        "\n",
        "Documents are split into overlapping text chunks, embedded into dense vectors, and stored in a LlamaIndex vector store to enable efficient similarity-based retrieval.\n",
        "\n",
        "Llama 3.1 (via Ollama) is configured for indexing-time language understanding,\n",
        "while dense embeddings are generated using a SentenceTransformer model.\n",
        "\n",
        "### Design Choices\n",
        "\n",
        "- **Chunk size (150 tokens, 20 overlap):** balances factual containment with retrieval recall  \n",
        "- **Document-level metadata:** enables attribution of answers to specific Prime Ministers  \n",
        "- **Dense embeddings:** improve semantic recall over keyword-based retrieval  \n",
        "- **Index persistence:** ensures reproducibility and faster iteration during evaluation\n"
      ],
      "metadata": {
        "id": "xZNE8C9lWekI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Embedding Model Setup\n",
        "# ------------------------------------------------------\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "stBaRUMCfQGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama 3.1 is used only during indexing-time processing within LlamaIndex.\n",
        "All downstream answer generation is performed using Flan-T5-Large."
      ],
      "metadata": {
        "id": "LepGu30qiHGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# LLM setup for indexing\n",
        "# ------------------------------------------------------\n",
        "llm = Ollama(model=\"llama-3.1-8b-instant\")\n",
        "Settings.llm = llm"
      ],
      "metadata": {
        "id": "bUYasywGfS-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each chunk is tagged with a document_id corresponding to the Prime Minister’s name.\n",
        "This enables traceability of retrieved context during evaluation and error analysis.\n"
      ],
      "metadata": {
        "id": "BLcFLmFmArD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Text chunking configuration\n",
        "# ------------------------------------------------------\n",
        "splitter = TokenTextSplitter(chunk_size=150, chunk_overlap=20)"
      ],
      "metadata": {
        "id": "M31_VAHvjZ2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Adding Metadata To Chunks\n",
        "# ------------------------------------------------------\n",
        "indexed_docs = []\n",
        "for name in pm_names:\n",
        "    page = wiki.page(name)\n",
        "    if page.exists():\n",
        "        text_chunks = splitter.split_text(page.text)\n",
        "        for chunk in text_chunks:\n",
        "            indexed_docs.append(Document(text=chunk, metadata={\"document_id\": name}))\n",
        "\n",
        "print(f\"Created {len(indexed_docs)} chunked documents with metadata tags.\")"
      ],
      "metadata": {
        "id": "zEa7lAArNs3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We build the VectorStoreIndex using all the chunked and embedded documents.\n",
        "The index is persisted locally to ensure reproducibility and to avoid recomputation during downstream retrieval experiments.\n",
        "\n",
        "\n",
        "> **Note:** Chunk-level metadata enables attribution of answers to specific Prime Minister documents during retrieval analysis.\n"
      ],
      "metadata": {
        "id": "iZaThsYIA5ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Indexing\n",
        "# ------------------------------------------------------\n",
        "index = VectorStoreIndex.from_documents(indexed_docs)\n",
        "print(\"Index created successfully.\")"
      ],
      "metadata": {
        "id": "SEUpUmYkfYZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Persist the index\n",
        "# ------------------------------------------------------\n",
        "if not os.path.exists(\"./pm_index\"):\n",
        "    os.makedirs(\"./pm_index\")\n",
        "index.storage_context.persist(persist_dir=\"./pm_index\")"
      ],
      "metadata": {
        "id": "izrmYZ-VsJxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG-Based Answering of Political Quiz Questions\n",
        "\n",
        "This section builds on the persisted vector index created earlier. The index is loaded and used to retrieve relevant Wikipedia passages for answering political quiz questions using a Retrieval-Augmented Generation (RAG) pipeline.\n",
        "\n",
        "The focus here is on:\n",
        "- Retrieval strategies (Top-K, query expansion, reranking)\n",
        "- Prompt engineering and ensembling\n",
        "- Quantitative evaluation using lexical and semantic metrics\n"
      ],
      "metadata": {
        "id": "O5inIzwmah0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimental Setup\n",
        "\n",
        "We set a random seed for reproducibility, specify the device (CPU/GPU), and define all configuration parameters:\n",
        "\n",
        "* LLM model → google/flan-t5-large\n",
        "* Embedding model → sentence-transformers/all-mpnet-base-v2\n",
        "* Paths for index (pm_index.zip) and QA file (QA.xlsx)\n",
        "\n",
        "Feature toggles (semantic reranking, WH-decomposition, few-shot prompting) allow controlled ablation of individual enhancements."
      ],
      "metadata": {
        "id": "NUc7cgJTiYzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Configuration and reproducibility\n",
        "# ------------------------------------------------------\n",
        "SEED = 38\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Model & embedding names (matching the earlier one)\n",
        "LLM_MODEL_NAME = \"google/flan-t5-large\"            # inference LLM (Flan-T5)\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"  # embedding (same as index)\n",
        "\n",
        "# Paths\n",
        "INDEX_ZIP = \"pm_index.zip\"      # produced earlier\n",
        "PERSIST_DIR = \"./pm_index\"\n",
        "QA_FILE = \"QA.xlsx\"\n",
        "OUTPUT_XLSX = \"RAG_Results_Final.xlsx\"\n",
        "\n",
        "# Toggle features (set to False to turn off)\n",
        "USE_SEMANTIC_RERANK = True         # rerank retrieved nodes using sentence-transformers\n",
        "USE_WH_DECOMPOSITION = True        # decompose query into what/when/where/how/who and run expansions\n",
        "USE_FEWSHOT_FROM_PERFECT = True    # derive few-shot examples automatically from perfect answered Qs\n",
        "USE_PROMPT_ENSEMBLE = True         # run multiple prompt templates and ensemble answers\n",
        "USE_CONFIDENCE_CALIBRATION = True  # compute/threshold confidence; optionally re-query if low\n",
        "USE_QUERY_EXPANSION = True         # add keywords / NER tokens to query before retrieval\n",
        "MAX_FEW_SHOTS = 3                  # how many few-shot examples to inject"
      ],
      "metadata": {
        "id": "8V4GxxBQes8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load Flan-T5-Large for answer generation and all-mpnet-base-v2 for semantic similarity. They both provide contextual understanding and dense vector embeddings used in RAG retrieval."
      ],
      "metadata": {
        "id": "uVAAgNwTC9JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Loading LLM and embeddings\n",
        "# ------------------------------------------------------\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_NAME, device_map=\"auto\",\n",
        "                                             torch_dtype=torch.float16 if device==\"cuda\" else torch.float32)\n",
        "llm = HuggingFaceLLM(model=model, tokenizer=tokenizer, max_new_tokens=128, context_window=1024)\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)\n",
        "\n",
        "# sentence-transformer for semantic rerank & similarity\n",
        "semantic_model = SentenceTransformer(EMBED_MODEL_NAME, device=device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eZ3jeDnze7sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then loaded the stored vector index (pm_index.zip) created earlier using LlamaIndex’s StorageContext. This index contains all chunked and embedded Wikipedia pages of Indian Prime Ministers. We then print the total number of documents in the store to confirm successful loading."
      ],
      "metadata": {
        "id": "yolvDsGtDOye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Load persisted index\n",
        "# ------------------------------------------------------\n",
        "\n",
        "if not os.path.exists(PERSIST_DIR):\n",
        "    # unzip if zip provided\n",
        "    if os.path.exists(INDEX_ZIP):\n",
        "        # Ensure the directory exists before unzipping\n",
        "        os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "        # Unzip into the PERSIST_DIR\n",
        "        !unzip -o {INDEX_ZIP} -d {PERSIST_DIR} > /dev/null\n",
        "    else:\n",
        "        raise FileNotFoundError(\"pm_index.zip or pm_index folder not found.\")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "index = load_index_from_storage(storage_context, embed_model=Settings.embed_model)\n",
        "print(\"Index loaded. Documents in store:\", len(index.docstore.docs))"
      ],
      "metadata": {
        "id": "O46sDZcJf5dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then loaded the provided QA.xlsx file containing quiz questions and their correct answers. And after cleaning and resetting indexes, the dataset is ready for RAG evaluation."
      ],
      "metadata": {
        "id": "KM8lD55TEEkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Load QA dataset and quick sanity\n",
        "# ------------------------------------------------------\n",
        "\n",
        "qa_df = pd.read_excel(QA_FILE)\n",
        "qa_df = qa_df.dropna(subset=[\"Question\", \"Answer\"]).reset_index(drop=True)\n",
        "print(\"Loaded QA rows:\", len(qa_df))\n",
        "print(qa_df.head())"
      ],
      "metadata": {
        "id": "B9MkZeOef9BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define helper functions for:\n",
        "\n",
        "* Text normalization → lowercasing and punctuation removal\n",
        "* Jaccard similarity → used as the primary accuracy metric\n",
        "* Semantic similarity → cosine similarity using sentence-transformers\n",
        "* Keyword extraction → extracting key nouns and named entities\n",
        "* WH-decomposition → identifying question types (“who”, “when”, “where”, \"how\") for query expansion\n",
        "\n",
        "This improved the query relevance and result accuracy."
      ],
      "metadata": {
        "id": "HVQJ65crEXN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Utility functions: normalization, jaccard, extract keywords\n",
        "# ------------------------------------------------------\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def normalize_text(s):\n",
        "    s = str(s).lower()\n",
        "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def jaccard_similarity(pred, truth):\n",
        "    A, B = set(normalize_text(pred).split()), set(normalize_text(truth).split())\n",
        "    return len(A & B) / len(A | B) if A or B else 0.0\n",
        "\n",
        "def semantic_similarity(pred, truth):\n",
        "    emb1 = semantic_model.encode(pred, convert_to_tensor=True)\n",
        "    emb2 = semantic_model.encode(truth, convert_to_tensor=True)\n",
        "    return float(util.cos_sim(emb1, emb2).item())\n",
        "\n",
        "def extract_keywords_from_question(q, top_k=5):\n",
        "    # lightweight keyword extraction: tokens minus stopwords, plus NER tokens\n",
        "    doc = nlp(q)\n",
        "    tokens = [tok.text.lower() for tok in doc if tok.is_alpha and tok.text.lower() not in stop_words]\n",
        "    # add named entities (PERSON, DATE, GPE)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ in (\"PERSON\",\"DATE\",\"GPE\",\"ORG\")]\n",
        "    candidates = tokens + entities\n",
        "    # frequency-based selection\n",
        "    freq = {}\n",
        "    for t in candidates:\n",
        "        t = t.strip().lower()\n",
        "        if not t: continue\n",
        "        freq[t] = freq.get(t,0)+1\n",
        "    sorted_keys = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [k for k,_ in sorted_keys][:top_k]\n",
        "\n",
        "# WH-decomposition\n",
        "WH_TAGS = [\"who\",\"what\",\"when\",\"where\",\"why\",\"how\"]\n",
        "def wh_decompose(q):\n",
        "    q_l = q.lower()\n",
        "    out = []\n",
        "    for w in WH_TAGS:\n",
        "        if q_l.startswith(w) or (\" \" + w + \" \") in q_l:\n",
        "            out.append(w)\n",
        "    # fallback: use simple heuristics\n",
        "    if not out:\n",
        "        # check question words\n",
        "        tokens = q_l.split()\n",
        "        for w in WH_TAGS:\n",
        "            if w in tokens:\n",
        "                out.append(w)\n",
        "    return list(dict.fromkeys(out))  # unique"
      ],
      "metadata": {
        "id": "1q9ggAEBf91U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We build retrieval helper functions:\n",
        "\n",
        "* retrieve_with_expansion() → retrieves top K chunks while optionally expanding queries with keywords and WH-tags\n",
        "* semantic_rerank_nodes() → reranks retrieved chunks based on cosine similarity between query and document embeddings\n",
        "\n",
        "This ensures the most contextually relevant paragraphs are passed to the LLM."
      ],
      "metadata": {
        "id": "zayKjYVLEukb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Retriever helpers & reranking\n",
        "# ------------------------------------------------------\n",
        "\n",
        "def retrieve_with_expansion(index, query, top_k=3, expansion_tokens=None, mode=\"default\"):\n",
        "    \"\"\"\n",
        "    index: llama-index index\n",
        "    query: string\n",
        "    expansion_tokens: list of strings to append to query (keywords, NER, dates)\n",
        "    mode: passed into as_retriever(similarity_top_k=...)\n",
        "    returns: list of retrieved nodes\n",
        "    \"\"\"\n",
        "    if expansion_tokens:\n",
        "        exp = \" \".join(expansion_tokens)\n",
        "        augmented_query = query + \" \" + exp\n",
        "    else:\n",
        "        augmented_query = query\n",
        "\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "    # use retriever settings if available\n",
        "    nodes = retriever.retrieve(augmented_query)\n",
        "    return nodes\n",
        "\n",
        "def semantic_rerank_nodes(query, nodes, top_k=2):\n",
        "    q_emb = semantic_model.encode(query, convert_to_tensor=True)\n",
        "    scored = []\n",
        "    for n in nodes:\n",
        "        txt = n.text\n",
        "        emb = semantic_model.encode(txt, convert_to_tensor=True)\n",
        "        score = float(util.cos_sim(q_emb, emb))\n",
        "        scored.append((n, score))\n",
        "    scored.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [n for n, _ in scored[:top_k]]"
      ],
      "metadata": {
        "id": "hBMECX4VgI-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define three complementary prompt styles:\n",
        "\n",
        "* Direct prompt – strict factual answer from context\n",
        "* Concise prompt – short phrase / entity answers\n",
        "* WH-aware prompt – interprets “who / when / where” question intent\n",
        "\n",
        "Multiple prompts are ensembled, and the model’s predictions are merged by majority vote or semantic confidence."
      ],
      "metadata": {
        "id": "-DGA6iUbFBYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Prompt templates & ensemble\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# We create multiple prompt templates; later we ensemble answers by voting or by similarity to context.\n",
        "\n",
        "PROMPTS = [\n",
        "    # conservative direct-answer prompt\n",
        "    (\"direct\",\n",
        "     \"You are a factual assistant specializing in Indian political history.\\n\"\n",
        "     \"Answer strictly using only the provided context. If the context lacks the answer, reply exactly: 'Not available in the context.'\\n\\n\"\n",
        "     \"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"),\n",
        "    # instruct to output concise phrase or name\n",
        "    (\"concise\",\n",
        "     \"You are a factual assistant. Use only the context below. Provide a concise answer (a phrase or short sentence). If not present, say: 'Not available in the context.'\\n\\n\"\n",
        "     \"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"),\n",
        "    # explicit WH-aware phrasing\n",
        "    (\"wh\",\n",
        "     \"You are an assistant specialized in history. Read the context and answer the question. If question asks 'who', respond with person(s); 'when' -> date/year; 'where' -> place. If missing: 'Not available in the context.'\\n\\n\"\n",
        "     \"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\")]\n",
        "\n",
        "def generate_answer_from_prompt(prompt_text, question, context):\n",
        "    prompt = prompt_text.format(context=context, question=question)\n",
        "    # tokenize + generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
        "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    if not pred:\n",
        "        pred = \"Empty Response\"\n",
        "    return pred"
      ],
      "metadata": {
        "id": "9PEsSz8cgZ3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We automatically identify “perfect” question–answer pairs (with Jaccard = 1 or semantic similarity > 0.98) and store them as few-shot examples.\n",
        "These examples are injected into subsequent prompts to guide the model toward higher factual consistency."
      ],
      "metadata": {
        "id": "j5eZTc9BFXi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Build few-shot example bank from \"100% correct\" QA items\n",
        "# ------------------------------------------------------\n",
        "# Strategy:\n",
        "# - Run a light retrieval+generation pass (TopK=1)\n",
        "# - Mark those with Jaccard==1 OR semantic similarity > 0.98 as perfect\n",
        "# - Use up to MAX_FEW_SHOTS such pairs (Q/A) as few-shot examples to inject into prompts.\n",
        "\n",
        "def build_perfect_example_bank(sample_questions, index, max_examples=3):\n",
        "    perfect_examples = []\n",
        "    for q, a in sample_questions:\n",
        "        nodes = retrieve_with_expansion(index, q, top_k=1)\n",
        "        context = \"\\n\\n\".join([n.text for n in nodes])\n",
        "        pred = generate_answer_from_prompt(PROMPTS[0][1], q, context)\n",
        "        jac = jaccard_similarity(pred, a)\n",
        "        sem = semantic_similarity(pred, a)\n",
        "        if jac == 1.0 or sem > 0.98:\n",
        "            perfect_examples.append({\"Question\": q, \"Answer\": a})\n",
        "        if len(perfect_examples) >= max_examples:\n",
        "            break\n",
        "    return perfect_examples\n",
        "\n",
        "# Build few-shot examples if toggle is ON\n",
        "if USE_FEWSHOT_FROM_PERFECT:\n",
        "    sample_qs = [(row.Question, row.Answer) for _, row in qa_df.head(100).iterrows()]\n",
        "    few_shots_bank = build_perfect_example_bank(sample_qs, index, max_examples=MAX_FEW_SHOTS)\n",
        "    print(\"Derived few-shot examples:\", len(few_shots_bank))\n",
        "    for ex in few_shots_bank:\n",
        "        print(\"Q:\", ex[\"Question\"], \"\\nA:\", ex[\"Answer\"])\n",
        "else:\n",
        "    few_shots_bank = []\n",
        "\n",
        "# Helper to format few-shot injection into prompt\n",
        "def get_few_shot_text(examples):\n",
        "    if not examples:\n",
        "        return \"\"\n",
        "    lines = []\n",
        "    for ex in examples:\n",
        "        lines.append(f\"Example - Q: {ex['Question']}\\nA: {ex['Answer']}\")\n",
        "    return \"\\n\\n\".join(lines) + \"\\n\\n\""
      ],
      "metadata": {
        "id": "ewe2tEPCzNwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Protocol\n",
        "\n",
        "All models are evaluated using:\n",
        "- **Jaccard Accuracy** for strict lexical overlap (assignment requirement)\n",
        "- **Semantic Accuracy** using cosine similarity\n",
        "- **Confidence score** based on answer–context alignment\n",
        "\n",
        "Results are reported separately for TopK = 1 and TopK = 2 retrieval settings.\n",
        "\n",
        "Flan-T5-Large was selected for its strong instruction-following behavior and stable factual generation under constrained context.\n"
      ],
      "metadata": {
        "id": "-T-MjEvMi6rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline RAG\n",
        "This baseline establishes a reference point using vanilla retrieval and a single prompt, against which all enhanced techniques are compared.\n"
      ],
      "metadata": {
        "id": "6Ls96wGxu6Y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first implemented a baseline RAG pipeline using the index created earlier which uses:\n",
        "\n",
        "* Direct retrieval from the Prime Minister Wikipedia index\n",
        "* A single simple prompt\n",
        "\n",
        "For each question in QA.xlsx, we retrieved:\n",
        "\n",
        "* TopK = 1 document\n",
        "* TopK = 2 documents\n",
        "\n",
        "The retrieved text was passed to the LLM to generate an answer.\n",
        "Accuracy was computed using Jaccard similarity, as required. For TopK = 1, I also recorded which PM document was retrieved."
      ],
      "metadata": {
        "id": "LErrueGIx0Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# BASELINE RAG\n",
        "# TopK = 1 and TopK = 2 WITHOUT enhancements\n",
        "# ------------------------------------------------------\n",
        "\n",
        "baseline_results = []\n",
        "\n",
        "for top_k in [1, 2]:\n",
        "    print(f\"\\n=== Running BASELINE RAG: TopK = {top_k} ===\")\n",
        "\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "\n",
        "    for idx, row in tqdm(qa_df.iterrows(), total=len(qa_df)):\n",
        "        q = row[\"Question\"]\n",
        "        true_ans = str(row[\"Answer\"])\n",
        "\n",
        "        # --- Simple retrieval ---\n",
        "        nodes = retriever.retrieve(q)\n",
        "        context = \"\\n\\n\".join([n.text for n in nodes])\n",
        "\n",
        "        # --- Simple prompt (baseline) ---\n",
        "        prompt = (\n",
        "            \"Use ONLY the context below to answer the question. \"\n",
        "            \"If the answer is not in the context, say 'Not available in the context.'\\n\\n\"\n",
        "            f\"Context:\\n{context}\\n\\nQuestion: {q}\\nAnswer:\"\n",
        "        )\n",
        "\n",
        "        pred = generate_answer_from_prompt(prompt, q, context)\n",
        "\n",
        "        # --- Clean prediction ---\n",
        "        pred = pred.strip()\n",
        "\n",
        "        # --- Jaccard metric ---\n",
        "        jacc = jaccard_similarity(pred, true_ans)\n",
        "\n",
        "        # --- For TopK=1 only: store retrieved document ---\n",
        "        retrieved_doc = nodes[0].metadata.get(\"document_id\", \"Unknown\") if top_k == 1 else None\n",
        "\n",
        "        baseline_results.append({\n",
        "            \"Question\": q,\n",
        "            \"True Answer\": true_ans,\n",
        "            \"Predicted Answer\": pred,\n",
        "            \"Jaccard Accuracy\": jacc,\n",
        "            \"TopK\": top_k,\n",
        "            \"Retrieved Doc (TopK=1 only)\": retrieved_doc\n",
        "        })\n",
        "\n",
        "baseline_df = pd.DataFrame(baseline_results)\n",
        "\n",
        "print(\"\\n=== Baseline RAG Summary ===\")\n",
        "print(baseline_df.groupby(\"TopK\")[\"Jaccard Accuracy\"].mean())"
      ],
      "metadata": {
        "id": "Qt3a_Hnku0ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_df.to_excel(\"Baseline_RAG_Results.xlsx\", index=False)\n",
        "print(\"Saved baseline results to Baseline_RAG_Results.xlsx\")"
      ],
      "metadata": {
        "id": "_SchSxdp3_9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enhanced RAG"
      ],
      "metadata": {
        "id": "mt5Jr-eyuvV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each question in the dataset and for both Top K = 1 and Top K = 2:\n",
        "1. We expand the query with keywords + WH-tags\n",
        "2. Then retrieve top chunks from the index\n",
        "3. Then apply semantic reranking\n",
        "4. Then build a few-shot prompt with selected context\n",
        "5. Then generate answers using the LLM\n",
        "6. Then move to build ensemble multiple prompt outputs\n",
        "7. Then compute Jaccard Accuracy, Semantic Accuracy, and Confidence\n",
        "\n",
        "Results for each question are stored along with the retrieved document IDs."
      ],
      "metadata": {
        "id": "yu7nEUa0FyBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# RAG loop (main)\n",
        "# ------------------------------------------------------\n",
        "\n",
        "results = []\n",
        "failure_cases = []\n",
        "\n",
        "TOPK_OPTIONS = [1,2]   # per assignment\n",
        "\n",
        "for top_k in TOPK_OPTIONS:\n",
        "    print(f\"\\n=== Running enhanced RAG: TopK = {top_k} ===\")\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "    for idx, row in tqdm(qa_df.iterrows(), total=len(qa_df), desc=f\"TopK={top_k}\"):\n",
        "        q = row[\"Question\"]\n",
        "        true_ans = str(row[\"Answer\"])\n",
        "\n",
        "        # --- Query decomposition & expansion --------------------------------\n",
        "        expansion_tokens = []\n",
        "        if USE_WH_DECOMPOSITION:\n",
        "            wh_tags = wh_decompose(q)\n",
        "            expansion_tokens += wh_tags\n",
        "        if USE_QUERY_EXPANSION:\n",
        "            kws = extract_keywords_from_question(q, top_k=6)\n",
        "            expansion_tokens += kws\n",
        "\n",
        "        # --- Initial retrieval  --------------------\n",
        "        retrieved_nodes = retrieve_with_expansion(index, q, top_k=max(5, top_k), expansion_tokens=expansion_tokens)\n",
        "\n",
        "        # if retrieval returns fewer nodes, guard\n",
        "        if not retrieved_nodes:\n",
        "\n",
        "            # fallback: raw retrieval without expansion\n",
        "            retrieved_nodes = retrieve_with_expansion(index, q, top_k=max(5, top_k), expansion_tokens=None)\n",
        "\n",
        "        # --- Semantic rerank -----------------------------------\n",
        "        if USE_SEMANTIC_RERANK:\n",
        "            reranked_nodes = semantic_rerank_nodes(q, retrieved_nodes, top_k=2)\n",
        "        else:\n",
        "            reranked_nodes = retrieved_nodes[:2]\n",
        "\n",
        "        context = \"\\n\\n\".join([n.text for n in reranked_nodes])\n",
        "\n",
        "        # --- Building prompts including few-shot examples -------------------\n",
        "        few_shot_text = get_few_shot_text(few_shots_bank) if USE_FEWSHOT_FROM_PERFECT else \"\"\n",
        "\n",
        "        # iterating over prompt ensemble and collect answers\n",
        "        answers = []\n",
        "        for pt_name, pt_text in PROMPTS if USE_PROMPT_ENSEMBLE else [PROMPTS[0]]:\n",
        "            prompt_body = f\"{few_shot_text}{pt_text}\"\n",
        "            pred = generate_answer_from_prompt(prompt_body, q, context)\n",
        "            answers.append({\"prompt\": pt_name, \"answer\": pred})\n",
        "\n",
        "        # --- Ensemble answers --------------------------\n",
        "        # simple majority vote on normalized strings; if tie, choose the one with highest\n",
        "        # average semantic similarity to context.\n",
        "        norm_answers = [normalize_text(a[\"answer\"]) for a in answers]\n",
        "\n",
        "        # majority vote\n",
        "        vote_counts = {}\n",
        "        for a in norm_answers:\n",
        "            vote_counts[a] = vote_counts.get(a,0)+1\n",
        "        voted_answer = max(vote_counts.items(), key=lambda x: (x[1], -len(x[0])))[0]\n",
        "\n",
        "        # if no clear vote or low confidence, pick answer with max semantic similarity to context\n",
        "        if USE_CONFIDENCE_CALIBRATION:\n",
        "\n",
        "            # compute similarity between each predicted answer and retrieved context\n",
        "            ctx_emb = semantic_model.encode(context, convert_to_tensor=True)\n",
        "            best = None\n",
        "            best_score = -1.0\n",
        "            for a in answers:\n",
        "                ans_text = a[\"answer\"]\n",
        "                ans_emb = semantic_model.encode(ans_text, convert_to_tensor=True)\n",
        "                score = float(util.cos_sim(ans_emb, ctx_emb))\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best = a[\"answer\"]\n",
        "\n",
        "            # pick the more confident between voted_answer and best by comparing their scores\n",
        "            voted_emb = semantic_model.encode(voted_answer, convert_to_tensor=True)\n",
        "            voted_score = float(util.cos_sim(voted_emb, ctx_emb))\n",
        "\n",
        "            # adopt the answer with higher context similarity\n",
        "            final_answer = best if best_score >= voted_score else voted_answer\n",
        "            confidence = max(best_score, voted_score)\n",
        "        else:\n",
        "            final_answer = voted_answer\n",
        "            confidence = semantic_similarity(final_answer, context) if context.strip() else 0.0\n",
        "\n",
        "        # --- If confidence low, fallback: re-query with larger TopK or different expansion\n",
        "        if USE_CONFIDENCE_CALIBRATION and confidence < 0.35:\n",
        "            # try wider retrieval (TopK=5), rerank, re-generate with the same prompts (single attempt)\n",
        "            alt_nodes = retrieve_with_expansion(index, q, top_k=5, expansion_tokens=expansion_tokens)\n",
        "            alt_rerank = semantic_rerank_nodes(q, alt_nodes, top_k=3) if alt_nodes else reranked_nodes\n",
        "            alt_context = \"\\n\\n\".join([n.text for n in alt_rerank])\n",
        "            # regenerate with the single direct prompt\n",
        "            alt_pred = generate_answer_from_prompt(PROMPTS[0][1], q, alt_context)\n",
        "            alt_conf = semantic_similarity(alt_pred, alt_context)\n",
        "            # adopt alt_pred if it improves context similarity\n",
        "            if alt_conf > confidence:\n",
        "                final_answer = alt_pred\n",
        "                confidence = alt_conf\n",
        "                context = alt_context\n",
        "                reranked_nodes = alt_rerank\n",
        "\n",
        "        # --- Final normalization (strip leading text like \"Answer:\" or \"The answer is\") ---\n",
        "        final_answer = re.sub(r'^(answer[:\\-\\s]*)', '', final_answer, flags=re.I).strip()\n",
        "        final_answer = re.sub(r'^(the answer is[:\\-\\s]*)', '', final_answer, flags=re.I).strip()\n",
        "\n",
        "        # --- Evaluation metric (Jaccard similarity) -------------\n",
        "        jacc = jaccard_similarity(final_answer, true_ans)\n",
        "        sem_acc = semantic_similarity(final_answer, true_ans)\n",
        "\n",
        "        top_docs_report = [n.metadata.get(\"document_id\", \"Unknown\") for n in reranked_nodes]\n",
        "        results.append({\n",
        "            \"Question\": q,\n",
        "            \"True Answer\": true_ans,\n",
        "            \"Predicted Answer\": final_answer,\n",
        "            \"Jaccard Accuracy\": jacc,\n",
        "            \"Semantic Accuracy\": sem_acc,\n",
        "            \"Confidence\": confidence,\n",
        "            \"TopK\": top_k,\n",
        "            \"Retrieved Docs\": top_docs_report\n",
        "        })\n",
        "\n",
        "        # collect failure case for manual inspection if jacc < threshold\n",
        "        if jacc < 0.4:\n",
        "            failure_cases.append({\n",
        "                \"Question\": q,\n",
        "                \"True Answer\": true_ans,\n",
        "                \"Predicted Answer\": final_answer,\n",
        "                \"Jaccard\": jacc,\n",
        "                \"Confidence\": confidence,\n",
        "                \"Retrieved Docs\": top_docs_report,\n",
        "                \"Context\": context})"
      ],
      "metadata": {
        "id": "htmswmIndyiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then aggregate all results and compute average metrics grouped by Top K.\n",
        "\n",
        "\n",
        "| **Metric**  | **Description** |\n",
        "|-------------|-----------------|\n",
        "| Jaccard Accuracy  | Number of overlapping words / Number of unique words between predicted and true answers |\n",
        "| Semantic Accuracy | Cosine similarity between embedding representations |\n",
        "| Confidence     | Similarity between generated answer and context embeddings |\n",
        "\n",
        "\n",
        "The summary table reports average Jaccard and Confidence scores for Top K = 1 and Top K = 2."
      ],
      "metadata": {
        "id": "zuTukeyUGqWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Results export & summary\n",
        "# ------------------------------------------------------\n",
        "res_df = pd.DataFrame(results)\n",
        "display(res_df.head())"
      ],
      "metadata": {
        "id": "ZZ2Sowrio_nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary by TopK\n",
        "summary = res_df.groupby(\"TopK\")[[\"Jaccard Accuracy\", \"Semantic Accuracy\", \"Confidence\"]].mean().reset_index()\n",
        "print(\"\\nAverage metrics by TopK:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "jHATQ1jrpYGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to excel\n",
        "res_df.to_excel(OUTPUT_XLSX, index=False)\n",
        "print(f\"\\nResults saved to {OUTPUT_XLSX}\")"
      ],
      "metadata": {
        "id": "t-UfTibEpaY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reproducibility Notes\n",
        "\n",
        "All experiments are fully reproducible given the persisted index, fixed random seeds, and deterministic decoding settings.\n"
      ],
      "metadata": {
        "id": "XoV_0Ks4kwwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a complete RAG pipeline with progressive\n",
        "enhancements over a baseline system. Results show consistent gains\n",
        "from query expansion, reranking, prompt ensembling, and confidence calibration.\n"
      ],
      "metadata": {
        "id": "sdXfkAUPcxQM"
      }
    }
  ]
}